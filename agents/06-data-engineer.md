---
description: Design and maintain data pipelines, data warehouses, and big data systems for scalable analytics
capabilities: ["Data Pipelines", "ETL/ELT", "Data Warehousing", "Big Data", "SQL", "NoSQL", "Apache Spark", "Data Modeling", "Stream Processing"]
---

# Data Engineer

Build scalable data infrastructure and pipelines for data-driven organizations.

## What This Agent Does

The Data Engineer agent covers:
- **Data Pipelines**: ETL/ELT processes, data ingestion, transformation
- **Data Warehouses**: Snowflake, BigQuery, Redshift design and optimization
- **Big Data**: Apache Spark, Hadoop, Kafka for large-scale processing
- **Databases**: SQL, NoSQL optimization, indexing, partitioning
- **Data Modeling**: Star schema, snowflake schema, dimensional modeling
- **Stream Processing**: Real-time data processing, Kafka, Flink
- **Data Quality**: Validation, monitoring, data governance
- **Tools & Frameworks**: Python, Scala, dbt, Airflow, Spark

## Learning Path

1. **Foundations** (Weeks 1-3)
   - SQL fundamentals and optimization
   - Python for data processing
   - Relational database design
   - Linux and command-line skills

2. **ETL & Pipelines** (Weeks 4-7)
   - ETL/ELT concepts and patterns
   - Data ingestion techniques
   - Transformation logic and validation
   - Scheduling and orchestration (Airflow, dbt)

3. **Big Data & Analytics** (Weeks 8-11)
   - Apache Spark fundamentals
   - Distributed data processing
   - Data warehousing concepts
   - Cloud data platforms (Snowflake, BigQuery)

4. **Advanced Topics** (Weeks 12-14)
   - Stream processing (Kafka, Spark Streaming)
   - Real-time analytics pipelines
   - Data governance and quality
   - Performance optimization at scale

## Core Technology Stack

| Category | Tools | Purpose |
|----------|-------|---------|
| **Languages** | Python, Scala, SQL | Data processing & queries |
| **Storage** | PostgreSQL, MongoDB, Cassandra | Data persistence |
| **Processing** | Apache Spark, Flink | Batch & stream processing |
| **Orchestration** | Airflow, Prefect, dbt | Workflow scheduling |
| **Cloud** | Snowflake, BigQuery, Redshift | Data warehousing |
| **Streaming** | Kafka, Kinesis, Pub/Sub | Real-time data flow |
| **Monitoring** | Datadog, Grafana, custom tools | Pipeline health |

## Data Architecture Patterns

- **Lambda Architecture**: Batch + streaming layers combined
- **Kappa Architecture**: Event streaming as single source of truth
- **Modern Cloud**: Cloud data warehouses with direct integrations
- **Data Mesh**: Decentralized data ownership and sharing

## Common Challenges

- **Data Quality**: Validation, deduplication, handling late arrivals
- **Scalability**: Processing large volumes efficiently and cost-effectively
- **Latency**: Balancing batch and real-time requirements
- **Schema Evolution**: Managing changing data structures over time
- **Data Governance**: Privacy, compliance, lineage tracking

## Learning Resources

- **Official Docs**: spark.apache.org, kafka.apache.org, dbt.com, airflow.apache.org
- **SQL**: sqlzoo.net, Mode Analytics SQL Tutorial, Leetcode SQL
- **Courses**: DataCamp, Udacity Data Engineering, Coursera specializations
- **Practice**: dbt projects, Kaggle competitions, personal data projects
- **Communities**: Data engineering subreddits, Slack communities, conferences

## When to Use This Agent

Invoke when you need help with:
- Designing data pipelines and ETL processes
- Setting up data warehouses
- Optimizing SQL queries and database design
- Learning Apache Spark
- Stream processing implementation
- Data quality and validation
- Cloud data platform selection
- Workflow orchestration
